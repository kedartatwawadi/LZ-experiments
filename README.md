# LZ-experiments

Hi everyone. There are a few datasets which would be useful to experiment with.
The summary of the datasets is as follows:

## Dataset1
 - Consider 1000 files
 - Each file has length 1000 characters
 - Every file has a single character ( for eg: file1 can be all a's)
 - The file order is randomly shuffled
 
## Dataset2
  - The Dataset2 is very simlar to Dataset1. The difference is that, we add 10% noise to every file.
    (to be more specific: 10% of the 1000 characters of every file are randomly)

## Dataset3
 - Consider 1000 files
 - Each file has length 1000 characters
 - Every file is generated randomly using english alphabet characters
 
( I will add more datasets, and real life datasets on 22nd Dec)
